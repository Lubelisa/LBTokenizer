{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LBTokenizer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "16b9049d3a164b4e98d047bb66d9a3b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FileUploadModel",
          "state": {
            "_view_name": "FileUploadView",
            "_counter": 1,
            "style": "IPY_MODEL_b6ce1df378d64ec481612d4cee8b80b2",
            "_dom_classes": [],
            "description": "Upload",
            "multiple": false,
            "_model_name": "FileUploadModel",
            "data": [
              null
            ],
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "accept": ".txt",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "error": "",
            "description_tooltip": null,
            "metadata": [
              {
                "name": "noticia_ontem.txt",
                "type": "text/plain",
                "size": 1042,
                "lastModified": 1617168359857
              }
            ],
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3b838aefeef9412c87b557a38ac9f506",
            "icon": "upload"
          }
        },
        "b6ce1df378d64ec481612d4cee8b80b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3b838aefeef9412c87b557a38ac9f506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4697f497bea43ed83219cdcb5aa03db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "state": {
            "_view_name": "ButtonView",
            "style": "IPY_MODEL_2c64ccffa05d428db707225ede46e1b3",
            "_dom_classes": [],
            "description": "Tokenizar",
            "_model_name": "ButtonModel",
            "button_style": "",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "tooltip": "",
            "_view_count": null,
            "disabled": false,
            "_view_module_version": "1.5.0",
            "layout": "IPY_MODEL_01f9790e0e764ff2b71d44c1d70c202d",
            "_model_module": "@jupyter-widgets/controls",
            "icon": ""
          }
        },
        "2c64ccffa05d428db707225ede46e1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ButtonStyleModel",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "button_color": null,
            "font_weight": "",
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "01f9790e0e764ff2b71d44c1d70c202d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KEaNm6y0I5B"
      },
      "source": [
        "##LBTokenizer - versão apenas tokenizador"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY4ogJGeLTLq",
        "outputId": "4c0871cd-8bbe-4467-bada-df638d54d770"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "import re\n",
        "from ipywidgets import widgets\n",
        "from IPython.display import display\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 252,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.7/dist-packages (1.2.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 252
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCe5O4H1LV-5"
      },
      "source": [
        "def ord(k):\n",
        "\treturn int(k.replace('_',''))"
      ],
      "execution_count": 253,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtCZMcKALYub"
      },
      "source": [
        "# Obtendo abreviaturas e pronomes de tratamento contendo pontos e espacos\n",
        "list_1 = open('lexicos/abreviaturas_com_pontos_e_espacos_ord.txt', 'r').readlines()\n",
        "list_1 = [word.replace('\\n','') for word in list_1]\n",
        "dict_abreviaturas_com_pontos_e_espacos_expressoes = dict()\n",
        "dict_abreviaturas_com_pontos_e_espacos_numeros = dict()\n",
        "dict_abreviaturas_com_pontos_e_espacos_numeros_expressoescomoeram = dict()\n",
        "\n",
        "id_ = 1\n",
        "\n",
        "for word in list_1:\n",
        "\tword = word\n",
        "\tdict_abreviaturas_com_pontos_e_espacos_expressoes[word] = str('__' + str(id_) + '__')\n",
        "\tdict_abreviaturas_com_pontos_e_espacos_numeros[str('__' + str(id_) + '__')] = word\n",
        "\tdict_abreviaturas_com_pontos_e_espacos_numeros_expressoescomoeram[str('__' + str(id_) + '__')] = []\n",
        "\tid_ += 1\n",
        "\n",
        "# clíticos dos verbos conjugados em mesóclises no futuro do pretérito e do presente\n",
        "list_vf = open('lexicos/verbos_irregulares_cliticizados.txt', 'r').readlines()\n",
        "list_vf = [exp.replace('\\n', '') for exp in list_vf]\n",
        "\n",
        "# Tratando contrações de pron+artigo\n",
        "list_abv = open('lexicos/contraidos_pronprep_prondet.txt', 'r').readlines()\n",
        "list_abv = [word.replace('\\n', '') for word in list_abv]\n",
        "\n",
        "# Obtendo os pronomes\n",
        "list_p = ['me', 'te', 'se', 'lhe', 'o', 'a', 'nos', 'vos', 'lhes', 'os', 'as']\n",
        "list_t = ['ei', 'ás', 'á', 'emos', 'eis', 'ão', 'ia', 'ias', 'ia', 'íamos', 'íeis', 'iam']\n",
        "\n",
        "# Obtendo os pronomes contraídos\n",
        "list_pc = open('lexicos/pron_cliticos_contraidos.txt', 'r').readlines()\n",
        "list_pc = [pron.replace('\\n', '') for pron in list_pc]\n",
        "\n",
        "dict_cliticos_contraidos_expressoes = dict()\n",
        "\n",
        "for pron in list_pc:\n",
        "\tlpron = pron.split(',');\n",
        "\tdict_cliticos_contraidos_expressoes[lpron[0]] = lpron[1]"
      ],
      "execution_count": 254,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1BomSbzKixB"
      },
      "source": [
        "def LBTok_Function(frases, nameFile):\n",
        "  # frases = open(\"input.txt\", 'r').readlines()\n",
        "  for i in range(0, len(frases)):\n",
        "    frases[i] = frases[i].replace('\\n', '')\n",
        "    \n",
        "  nameFile = nameFile.replace('.txt', '_tokenizado.txt')\n",
        "  arq = open(nameFile, 'w', encoding='utf-8')\n",
        "  # arq = open('input_tokenizado.txt', 'w')\n",
        "\n",
        "\n",
        "  for i in range(0, len(frases)):\n",
        "    # print('Frase ' + str(i+1))\n",
        "\n",
        "    # Tratando abreviaturas e pronomes de tratamento contendo pontos e espacos (antes de tokenizar) ###############################\n",
        "\n",
        "    list_iter = []\n",
        "\n",
        "    for word in list_1:\n",
        "      text = frases[i].lower()\n",
        "      id_word = text.find(word)\n",
        "      Flag = False\n",
        "      while id_word != -1:\n",
        "        Flag = False\n",
        "        if word in text and (id_word == 0):\n",
        "          dict_abreviaturas_com_pontos_e_espacos_numeros_expressoescomoeram[dict_abreviaturas_com_pontos_e_espacos_expressoes[word]].append(frases[i][id_word:(id_word+len(word))])\n",
        "          list_iter.append(dict_abreviaturas_com_pontos_e_espacos_expressoes[word])\n",
        "          frases[i] = dict_abreviaturas_com_pontos_e_espacos_expressoes[word] + frases[i][(id_word+len(word)):len(frases[i])]\n",
        "          text = frases[i].lower()\n",
        "          Flag = True\n",
        "        else:\n",
        "          for pnt in [' ',',','!','?',';',':']:\n",
        "            if word in text and (id_word != 0 and text[id_word-1] == pnt):\n",
        "              # print('Entrou!!!')\n",
        "              dict_abreviaturas_com_pontos_e_espacos_numeros_expressoescomoeram[str(dict_abreviaturas_com_pontos_e_espacos_expressoes[word])].append(frases[i][id_word:(id_word+len(word))])\n",
        "              list_iter.append(dict_abreviaturas_com_pontos_e_espacos_expressoes[word])\n",
        "              frases[i] = frases[i][0:id_word] + dict_abreviaturas_com_pontos_e_espacos_expressoes[word] + frases[i][(id_word+len(word)):len(frases[i])]\n",
        "              text = frases[i].lower()\n",
        "              # print(text)\n",
        "              Flag = True\n",
        "        id_word = text.find(word)\n",
        "        if Flag == False:\n",
        "          id_word = -1\n",
        "\n",
        "    # Separando aspas simples ########################################################################################################\n",
        "\n",
        "    frases[i] = frases[i].replace('\\'', ' \\' ')\n",
        "    # frases[i] = frases[i].replace('\"', ' \" ')\n",
        "\n",
        "    frases[i] = frases[i].replace('  ', ' ')\n",
        "\n",
        "    # Procurando por acronimos #######################################################################################################\n",
        "\n",
        "    # print('Antes: ', frasprintes[i])\n",
        "\n",
        "    dict_siglas = dict()\n",
        "    list_siglas = []\n",
        "    id_ = 1\n",
        "\n",
        "    list_siglas = re.findall('(?:(?<=\\.|\\s)[A-Z]\\.)+', frases[i])\n",
        "    # print(list_siglas)\n",
        "\n",
        "    for j in range(len(list_siglas)):\n",
        "      frases[i] = frases[i].replace(list_siglas[j], '_+'+str(id_)+'+_')\n",
        "      dict_siglas['_+'+str(id_)+'+_'] = list_siglas[j]\n",
        "      id_+=1\n",
        "\n",
        "\n",
        "    # Tokenizando o texto usando a função word_tokenize ###########################################################################\n",
        "\n",
        "    palavras_frase = word_tokenize(frases[i], language='portuguese')\n",
        "    for j in range(len(palavras_frase)):\n",
        "      palavras_frase[j] = palavras_frase[j].replace('\\'\\'', '\"')\n",
        "      palavras_frase[j] = palavras_frase[j].replace('``', '\"')\n",
        "\n",
        "    # print(palavras_frase)\n",
        "\n",
        "    # Procurando por siglas #######################################################################################################\n",
        "\n",
        "    for j in range(len(palavras_frase)):\n",
        "      poss_sigla = re.findall('[A-ZÁÀÂÃÉÈÊÍÏÓÔÕÖÚÇÑ0123456789][A-ZÁÀÂÃÉÈÊÍÏÓÔÕÖÚÇÑ0123456789]+', palavras_frase[j])\n",
        "      # print(len(poss_sigla) > 0)\n",
        "      # print(poss_sigla[0] == len(palavras_frase[j]))\n",
        "      # print(poss_sigla)\n",
        "      if len(poss_sigla) > 0 and (poss_sigla[0] == palavras_frase[j]):\n",
        "        # print(id_)\n",
        "        palavras_frase[j] = '_+'+str(id_)+'+_'\n",
        "        dict_siglas['_+'+str(id_)+'+_'] = poss_sigla[0]\n",
        "        id_+=1\n",
        "\n",
        "    # print('Depois: ', palavras_frase)\n",
        "    # Verificando quais palavras da sentença iniciam com letra máiúscula e quais não\n",
        "\n",
        "    palavras_frase_maiusculas = ['L'] * len(palavras_frase)\n",
        "    \n",
        "    for j in range(len(palavras_frase)):\n",
        "      # print(palavras_frase[i])\n",
        "      # print(palavras_frase[i][0].isupper())\n",
        "      if palavras_frase[j][0].isupper() == True:\n",
        "        palavras_frase_maiusculas[j] = 'U'\n",
        "      else:\n",
        "        palavras_frase_maiusculas[j] = 'L'\n",
        "\n",
        "\n",
        "    # Criando outra versão da sentença em lowercase apa realizar o processamento ##################################################\n",
        "\n",
        "    palavras_tokenize = [palavras_frase[j].lower() for j in range(len(palavras_frase))]\n",
        "\n",
        "    # Tratando os clíticos dos verbos conjugados em mesóclises no futuro do pretérito e do presente (antes de tokenizar) ##########\n",
        "    for j in range(len(palavras_tokenize)):\n",
        "      for exp in list_vf:\n",
        "        exp = exp.split(',')\n",
        "        if palavras_tokenize[j] == exp[0]:\n",
        "          palavras_tokenize[j] = exp[1]\n",
        "\n",
        "    # Tratando contracoes de pronomes+artigos #####################################################################################\n",
        "\n",
        "    for id_ in range(0,len(palavras_tokenize)):\n",
        "      for word in list_abv:\n",
        "        word = word.split(',')\n",
        "        if palavras_tokenize[id_] == word[0]:\n",
        "          # print('oi')\n",
        "          # print(word[1])\n",
        "          palavras_tokenize[id_] = word[1]\n",
        "          # print(palavras_tokenize[id_])\n",
        "      \n",
        "    # Tratando clíticos (depois de tokenizar) #####################################################################################\n",
        "\n",
        "    for id_ in range(0,len(palavras_tokenize)):\n",
        "      # Caso seja enclise\n",
        "      if palavras_tokenize[id_].count('-') == 1:\n",
        "        posHifen = palavras_tokenize[id_].find('-')\n",
        "        pron = palavras_tokenize[id_][posHifen+1:len(palavras_tokenize[id_])]\n",
        "        if pron in list_p:\n",
        "          if pron in list_pc:\n",
        "            palavras_tokenize[id_] = palavras_tokenize[id_].replace('-'+pron, ' '+dict_cliticos_contraidos_expressoes[pron])\n",
        "          else:\n",
        "            palavras_tokenize[id_] = palavras_tokenize[id_].replace('-',' ')\n",
        "\n",
        "        else:\n",
        "          palavras_tokenize[id_] = palavras_frase[id_]\n",
        "\n",
        "      # Caso seja mesóclise\n",
        "      elif palavras_tokenize[id_].count('-') == 2:\n",
        "        # Encontrando os hifens e o possível pronome\n",
        "        primHifen = palavras_tokenize[id_].find('-')\n",
        "        segHifen = palavras_tokenize[id_].find('-', primHifen+1,len(palavras_tokenize[id_]))\n",
        "        pron = palavras_tokenize[id_][primHifen+1:segHifen]\n",
        "        term = palavras_tokenize[id_][segHifen+1:len(palavras_tokenize[id_])]\n",
        "\n",
        "        # print('pron: ', pron)\n",
        "        # print('term: ', term)\n",
        "\n",
        "        # Tratando se for mesóclise\n",
        "        if pron in list_p and term in list_t:\n",
        "          if palavras_tokenize[id_][primHifen-1] != 'r':\n",
        "            palavras_tokenize[id_] = palavras_tokenize[id_].replace('-'+pron+'-', 'r')\n",
        "          else:\n",
        "            palavras_tokenize[id_] = palavras_tokenize[id_].replace('-'+pron+'-', '')\n",
        "\n",
        "          if pron in dict_cliticos_contraidos_expressoes.keys():\n",
        "            palavras_tokenize[id_] += ' '+dict_cliticos_contraidos_expressoes[pron]\n",
        "          else:\n",
        "            palavras_tokenize[id_] += ' '+pron\n",
        "\n",
        "\n",
        "    # Recolocando os pronomes de tratamento (depois de tokenizar e tratar os casos) #################################################\n",
        "\n",
        "    for key_ in list_iter:\n",
        "      for id_ in range(len(palavras_tokenize)):\n",
        "        id_key = palavras_tokenize[id_].find(key_)\n",
        "        while id_key != -1:\n",
        "          palavras_tokenize[id_] = palavras_tokenize[id_][0:id_key]+dict_abreviaturas_com_pontos_e_espacos_numeros_expressoescomoeram[key_][0]+palavras_tokenize[id_][id_key+len(key_):len(palavras_tokenize[id_])]\n",
        "          dict_abreviaturas_com_pontos_e_espacos_numeros_expressoescomoeram[key_].pop(0)\n",
        "          id_key = palavras_tokenize[id_].find(key_)\n",
        "\n",
        "    for j in range(0, len(palavras_frase_maiusculas)):\n",
        "      if palavras_frase_maiusculas[j] == 'U':\n",
        "        palavras_tokenize[j] = palavras_tokenize[j][0].upper() + palavras_tokenize[j][1:len(palavras_tokenize[j])]\n",
        "\n",
        "    text = ' '.join(palavras_tokenize)\n",
        "\n",
        "    for key_ in dict_siglas.keys():\n",
        "      if key_ in text:\n",
        "        text = text.replace(key_, dict_siglas[key_])\n",
        "\n",
        "    text = text.replace('R $', 'R$')\n",
        "\n",
        "    # text = text.encode('utf-8')\n",
        "    arq.write(text+'\\n')\n",
        "\n",
        "  arq.close()"
      ],
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LaIy9FGhm4p"
      },
      "source": [
        "uploader = widgets.FileUpload(\n",
        "    accept='.txt',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
        "    multiple=False,  # True to accept multiple files upload else False\n",
        "    charset='utf-8'\n",
        ")\n",
        "button = widgets.Button(description='Tokenizar')"
      ],
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 115,
          "referenced_widgets": [
            "16b9049d3a164b4e98d047bb66d9a3b0",
            "b6ce1df378d64ec481612d4cee8b80b2",
            "3b838aefeef9412c87b557a38ac9f506",
            "e4697f497bea43ed83219cdcb5aa03db",
            "2c64ccffa05d428db707225ede46e1b3",
            "01f9790e0e764ff2b71d44c1d70c202d"
          ]
        },
        "id": "XvP9Ij9hM8qn",
        "outputId": "b21ef9e3-c2d1-4294-b63e-44ec5db54f86"
      },
      "source": [
        "print(r'Faça o upload do arquivo .txt que deseja tokenizar: ')\n",
        "display(uploader)\n",
        "display(button)\n",
        "\n",
        "def on_button_clicked(_):\n",
        "      # \"linking function with output\"\n",
        "  \n",
        "      # what happens when we press the button\n",
        "      name = list(uploader.value.keys())[0]\n",
        "      # # uploader.value[name]['content']\n",
        "\n",
        "      frases = str(uploader.value[name]['content'], 'utf-8')\n",
        "      frases = frases.split('\\n')\n",
        "      # clear_output()\n",
        "      LBTok_Function(frases, name)\n",
        "      print('Arquivo',name,'tonekizado!')\n",
        "\n",
        "button.on_click(on_button_clicked)\n"
      ],
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Faça o upload do arquivo .txt que deseja tokenizar: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16b9049d3a164b4e98d047bb66d9a3b0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "FileUpload(value={}, accept='.txt', description='Upload')"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4697f497bea43ed83219cdcb5aa03db",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "Button(description='Tokenizar', style=ButtonStyle())"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Arquivo noticia_ontem.txt tonekizado!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}